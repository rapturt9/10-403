{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np # NOTE only imported because https://github.com/pytorch/pytorch/issues/13918\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradient(nn.Module):\n",
    "    def __init__(self, state_size, action_size, lr_actor=1e-3, lr_critic=1e-3, mode='REINFORCE', n=128, gamma=0.99, device='cpu'):\n",
    "        super(PolicyGradient, self).__init__()\n",
    "\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.mode = mode\n",
    "        self.n = n\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        hidden_layer_size = 256\n",
    "\n",
    "        # actor\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_size, hidden_layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_layer_size, action_size),\n",
    "            # BEGIN STUDENT SOLUTION\n",
    "            nn.Softmax(dim=-1)\n",
    "            # END STUDENT SOLUTION\n",
    "        )\n",
    "\n",
    "        # critic\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_size, hidden_layer_size),\n",
    "            nn.ReLU(),\n",
    "            # BEGIN STUDENT SOLUTION\n",
    "            nn.Linear(hidden_layer_size, action_size),\n",
    "            nn.Softmax(dim=-1)\n",
    "            # END STUDENT SOLUTION\n",
    "        )\n",
    "\n",
    "        # initialize networks, optimizers, move networks to device\n",
    "        # BEGIN STUDENT SOLUTION\n",
    "        self.actor.to(self.device)\n",
    "        self.optimizer_actor = optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
    "        if ('REINFORCE' != self.mode):\n",
    "            self.critic.to(self.device)\n",
    "            self.optimizer_critic = optim.Adam(self.critic.parameters(), lr=lr_critic)\n",
    "        # END STUDENT SOLUTION\n",
    "\n",
    "\n",
    "    def forward(self, state):\n",
    "        if 'REINFORCE' == self.mode:\n",
    "            return(self.actor(state))\n",
    "        return(self.actor(state), self.critic(state))\n",
    "\n",
    "\n",
    "    def get_action(self, state, stochastic):\n",
    "        # if stochastic, sample using the action probabilities, else get the argmax\n",
    "        # BEGIN STUDENT SOLUTION\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "\n",
    "        if ('REINFORCE' != self.mode):\n",
    "            action_probs, _ = self.forward(state)\n",
    "        else:\n",
    "            action_probs = self.forward(state)\n",
    "\n",
    "        if stochastic:\n",
    "            distribution = torch.distributions.Categorical(action_probs)\n",
    "            action = distribution.sample()\n",
    "            return action.item()\n",
    "        else:\n",
    "            return torch.argmax(action_probs).item()\n",
    "        # END STUDENT SOLUTION\n",
    "\n",
    "\n",
    "    def calculate_n_step_bootstrap(self, rewards_tensor, values):\n",
    "        # calculate n step bootstrap\n",
    "        # BEGIN STUDENT SOLUTION\n",
    "        T = len(rewards_tensor)\n",
    "        N = self.n\n",
    "        Gs = []\n",
    "        for t in range(T):\n",
    "            if t + N < T:\n",
    "                V_end = values[t + N]\n",
    "            else:\n",
    "                V_end = 0\n",
    "            G_t = 0\n",
    "            upper_bound = min(t+N-1, T)\n",
    "            for k in range(t, upper_bound):\n",
    "                G_t += (self.gamma**(k-t)) * rewards_tensor[k]\n",
    "            G_t += V_end * (self.gamma**(N))\n",
    "            Gs.append(G_t)\n",
    "        return torch.FloatTensor(Gs).to(self.device)\n",
    "        # END STUDENT SOLUTION\n",
    "\n",
    "\n",
    "\n",
    "    #Custom reward calc function forreinforce and reinforce with baseline\n",
    "    def cust_reward_calc(self, rewards_tensor):\n",
    "        sz = len(rewards_tensor)\n",
    "        Gs = []\n",
    "        for t in range(sz):\n",
    "            G_t = 0\n",
    "            for k in range(t, sz):\n",
    "                G_t += (self.gamma**(k-t)) * rewards_tensor[k]\n",
    "            Gs.append(G_t)\n",
    "        return torch.FloatTensor(Gs).to(self.device)\n",
    "\n",
    "\n",
    "    def train(self, states, actions, rewards):\n",
    "        # train the agent using states, actions, and rewards\n",
    "        # BEGIN STUDENT SOLUTION\n",
    "\n",
    "        self.optimizer_actor.zero_grad()\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.LongTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "\n",
    "        if ('REINFORCE' == self.mode):\n",
    "            Gs = self.cust_reward_calc(rewards)\n",
    "        elif('REINFORCE_WITH_BASELINE' == self.mode):\n",
    "            self.optimizer_critic.zero_grad()\n",
    "            actor_probs, critic_values = self.forward(torch.FloatTensor(states).to(self.device))\n",
    "            critic_values = critic_values.gather(1, torch.LongTensor(actions).unsqueeze(1).to(self.device)).squeeze(1)\n",
    "            Gs = self.cust_reward_calc(rewards)\n",
    "        else:\n",
    "            self.optimizer_critic.zero_grad()\n",
    "            actor_probs, critic_values = self.forward(torch.FloatTensor(states).to(self.device))\n",
    "            critic_values = critic_values.gather(1, torch.LongTensor(actions).unsqueeze(1).to(self.device)).squeeze(1)\n",
    "            Gs = self.calculate_n_step_bootstrap(rewards, critic_values)\n",
    "\n",
    "        if ('REINFORCE' == self.mode):\n",
    "            log_prob = self.forward(torch.FloatTensor(states).to(self.device))\n",
    "            log_prob = log_prob.gather(1, torch.LongTensor(actions).unsqueeze(1).to(self.device)).squeeze(1).log()\n",
    "            loss = -1 * (log_prob * Gs).mean()\n",
    "            loss.backward()\n",
    "            self.optimizer_actor.step()\n",
    "        else:\n",
    "            actor_log_prob = actor_probs.gather(1, torch.LongTensor(actions).unsqueeze(1).to(self.device)).squeeze(1).log()\n",
    "            diff = Gs - critic_values\n",
    "            actor_loss = -1 * (diff * actor_log_prob).mean()\n",
    "            actor_loss.backward(retain_graph=True)\n",
    "            self.optimizer_actor.step()\n",
    "            critic_loss = (diff**2).mean()\n",
    "            critic_loss.backward()\n",
    "            self.optimizer_critic.step()\n",
    "\n",
    "\n",
    "        # END STUDENT SOLUTION\n",
    "\n",
    "\n",
    "\n",
    "    def run(self, env, max_steps, num_episodes, train):\n",
    "        total_rewards = []\n",
    "        # run the agent through the environment num_episodes times for at most max steps\n",
    "        # BEGIN STUDENT SOLUTION\n",
    "        for episode in range(num_episodes):\n",
    "            state = env.reset()[0]\n",
    "            states = []\n",
    "            actions = []\n",
    "            rewards = []\n",
    "            for step in range(max_steps):\n",
    "                action = self.get_action(state, train)\n",
    "                next_state, reward, done, info , _ = env.step(action)\n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    break\n",
    "            if train:\n",
    "                self.train(states, actions, rewards)\n",
    "            else:\n",
    "                total_rewards.append(sum(rewards))\n",
    "        # END STUDENT SOLUTION\n",
    "        return(total_rewards)\n",
    "\n",
    "\n",
    "\n",
    "def graph_agents(graph_name, agents, env, max_steps, num_episodes):\n",
    "    print(f'Starting: {graph_name}')\n",
    "\n",
    "    # graph the data mentioned in the homework pdf\n",
    "    # BEGIN STUDENT SOLUTION\n",
    "    atr = []\n",
    "    for agent in agents:\n",
    "        average_total_rewards_agent = []\n",
    "        iters = int(num_episodes/100)\n",
    "        for i in tqdm(range(iters)):\n",
    "            tr_train = agent.run(env, max_steps, 100, True)\n",
    "            tr_test = agent.run(env, max_steps, 20, False)\n",
    "            tr_test = sum(tr_test) / len(tr_test)\n",
    "            average_total_rewards_agent.append(tr_test)\n",
    "        atr.append(average_total_rewards_agent)\n",
    "\n",
    "    atr = torch.tensor(atr, dtype=torch.float)\n",
    "    min_values, _ = torch.min(atr, dim=0)\n",
    "    min_total_rewards = min_values.view(-1)\n",
    "    max_values, _ = torch.max(atr, dim=0)\n",
    "    max_total_rewards = max_values.view(-1)\n",
    "\n",
    "    average_total_rewards = torch.mean(atr, dim=0)\n",
    "    graph_every = 100\n",
    "    # END STUDENT SOLUTION\n",
    "\n",
    "    # plot the total rewards\n",
    "    xs = [i * graph_every for i in range(len(average_total_rewards))]\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.fill_between(xs, min_total_rewards.tolist(), max_total_rewards.tolist(), alpha=0.1)\n",
    "    ax.plot(xs, average_total_rewards)\n",
    "    ax.set_ylim(-max_steps * 0.01, max_steps * 1.1)\n",
    "    ax.set_title(graph_name, fontsize=10)\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Average Total Reward')\n",
    "    #fig.savefig(f'./graphs/{graph_name}.png')\n",
    "    #plt.close(fig)\n",
    "    plt.show()\n",
    "    print(f'Finished: {graph_name}')\n",
    "\n",
    "\n",
    "\n",
    "def main(mod, num_episodes, runs):\n",
    "    # init args, agents, and call graph_agents on the initialized agents\n",
    "    # BEGIN STUDENT SOLUTION\n",
    "    env = gym.make('CartPole-v1')\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    agents = []\n",
    "    for i in range(runs):\n",
    "        agents.append(PolicyGradient(state_size, action_size, mode=mod, n=64))\n",
    "    graph_agents(mod, agents, env, 200, num_episodes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RLHW1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
