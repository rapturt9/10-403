{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h]\n",
      "                             [--mode {REINFORCE,REINFORCE_WITH_BASELINE,A2C}]\n",
      "                             [--n N] [--num_runs NUM_RUNS]\n",
      "                             [--num_episodes NUM_EPISODES]\n",
      "                             [--max_steps MAX_STEPS] [--env_name ENV_NAME]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/Users/ram/Library/Jupyter/runtime/kernel-v2-34746VzymrkLiwIyG.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "#! python3\n",
    "\n",
    "import argparse\n",
    "\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np # NOTE only imported because https://github.com/pytorch/pytorch/issues/13918\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "class PolicyGradient(nn.Module):\n",
    "    def __init__(self, state_size, action_size, lr_actor=1e-3, lr_critic=1e-3, mode='REINFORCE', n=128, gamma=0.99, device='cpu'):\n",
    "        super(PolicyGradient, self).__init__()\n",
    "\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.mode = mode\n",
    "        self.n = n\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        hidden_layer_size = 256\n",
    "\n",
    "        # actor\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_size, hidden_layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_layer_size, action_size),\n",
    "            # BEGIN STUDENT SOLUTION\n",
    "            nn.Softmax(dim=-1)\n",
    "            # END STUDENT SOLUTION\n",
    "        )\n",
    "\n",
    "        # critic\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_size, hidden_layer_size),\n",
    "            nn.ReLU(),\n",
    "            # BEGIN STUDENT SOLUTION\n",
    "            # END STUDENT SOLUTION\n",
    "        )\n",
    "\n",
    "        # initialize networks, optimizers, move networks to device\n",
    "        # BEGIN STUDENT SOLUTION\n",
    "        self.reinforce = nn.Sequential(\n",
    "            nn.Linear(state_size, hidden_layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_layer_size, action_size),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "\n",
    "        #move to device\n",
    "        self.reinforce.to(device)\n",
    "        self.optimizer = optim.Adam(self.reinforce.parameters(), lr=lr_actor)\n",
    "        self.reinforce.to(device)\n",
    "        # END STUDENT SOLUTION\n",
    "\n",
    "\n",
    "    def forward(self, state):\n",
    "        if self.mode == 'REINFORCE':\n",
    "            return(self.reinforce(state))\n",
    "        return(self.actor(state), self.critic(state))\n",
    "\n",
    "\n",
    "    def get_action(self, state, stochastic):\n",
    "        # if stochastic, sample using the action probabilities, else get the argmax\n",
    "        # BEGIN STUDENT SOLUTION\n",
    "        state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "\n",
    "        probs = self.reinforce(state_tensor)\n",
    "        if stochastic:\n",
    "            distribution = torch.distributions.Categorical(probs)\n",
    "            action = distribution.sample()\n",
    "            return (probs, action.item())\n",
    "        else:\n",
    "            return (probs, torch.argmax(probs).item())\n",
    "        # END STUDENT SOLUTION\n",
    "\n",
    "\n",
    "    def calculate_n_step_bootstrap(self, rewards_tensor, values):\n",
    "        # calculate n step bootstrap\n",
    "        # BEGIN STUDENT SOLUTION\n",
    "\n",
    "        # END STUDENT SOLUTION\n",
    "        pass\n",
    "\n",
    "    def train(self, states, actions, rewards):\n",
    "        # BEGIN STUDENT SOLUTION\n",
    "        #actions = torch.tensor(actions, dtype=torch.int64, device=self.device)\n",
    "\n",
    "        # Calculate returns\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in rewards[::-1]:\n",
    "            G = r + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        returns = torch.tensor(returns, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        # Convert states to a tensor\n",
    "        states = torch.tensor(states, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        action_probs_tensor = torch.stack(actions)\n",
    "        log_probs = torch.log(action_probs_tensor)\n",
    "\n",
    "        loss = -torch.sum(log_probs * returns)\n",
    "        print(loss)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        # END STUDENT SOLUTION\n",
    "        pass\n",
    "\n",
    "    def run(self, env, max_steps, num_episodes, train):\n",
    "        total_rewards = []\n",
    "\n",
    "        # run the agent through the environment num_episodes times for at most max steps\n",
    "        # BEGIN STUDENT SOLUTION\n",
    "        for i in range(num_episodes):\n",
    "            state = env.reset()[0]\n",
    "            rewards = []\n",
    "            action_probs = []\n",
    "            for t in range(max_steps):\n",
    "                probs, action = self.get_action(state, True)\n",
    "                state, reward, done, done2, _ = env.step(action)\n",
    "                done = done or done2\n",
    "                rewards.append(reward)\n",
    "                action_prob = probs[0, action]\n",
    "                action_probs.append(action_prob)\n",
    "                if (done):\n",
    "                    max_steps = t\n",
    "                    break\n",
    "\n",
    "            if train:\n",
    "                self.train(state, action_probs, rewards)\n",
    "        # END STUDENT SOLUTION\n",
    "        return(total_rewards)\n",
    "\n",
    "\n",
    "\n",
    "def graph_agents(graph_name, agents, env, max_steps, num_episodes):\n",
    "    print(f'Starting: {graph_name}')\n",
    "\n",
    "    # graph the data mentioned in the homework pdf\n",
    "    # BEGIN STUDENT SOLUTION\n",
    "    total_episode_rewards = []\n",
    "    for i in range(5):\n",
    "      total_reward = agent.run(env, args.max_steps, args.num_episodes, True)\n",
    "      total_episode_rewards.append(total_reward)\n",
    "\n",
    "    transposed_array = list(zip(total_episode_rewards))\n",
    "    avgerage_total_rewards = avg_array = [sum(column) / len(column) for column in transposed_array]\n",
    "    min_total_rewards = [min(r) for r in zip(total_episode_rewards)]\n",
    "    max_total_rewards = [max(r) for r in zip(total_episode_rewards)]\n",
    "\n",
    "    graph_every = 100\n",
    "    # END STUDENT SOLUTION\n",
    "\n",
    "    # plot the total rewards\n",
    "    xs = [i * graph_every for i in range(len(average_total_rewards))]\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.fill_between(xs, min_total_rewards, max_total_rewards, alpha=0.1)\n",
    "    ax.plot(xs, average_total_rewards)\n",
    "    ax.set_ylim(-max_steps * 0.01, max_steps * 1.1)\n",
    "    ax.set_title(graph_name, fontsize=10)\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Average Total Reward')\n",
    "    fig.savefig(f'./graphs/{graph_name}.png')\n",
    "    plt.close(fig)\n",
    "    print(f'Finished: {graph_name}')\n",
    "\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    mode_choices = ['REINFORCE', 'REINFORCE_WITH_BASELINE', 'A2C']\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='Train an agent.')\n",
    "    parser.add_argument('--mode', type=str, default='REINFORCE', choices=mode_choices, help='Mode to run the agent in')\n",
    "    parser.add_argument('--n', type=int, default=64, help='The n to use for n step A2C')\n",
    "    parser.add_argument('--num_runs', type=int, default=5, help='Number of runs to average over for graph')\n",
    "    parser.add_argument('--num_episodes', type=int, default=3500, help='Number of episodes to train for')\n",
    "    parser.add_argument('--max_steps', type=int, default=200, help='Maximum number of steps in the environment')\n",
    "    parser.add_argument('--env_name', type=str, default='CartPole-v1', help='Environment name')\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    #args = parse_args()\n",
    "\n",
    "    # init args, agents, and call graph_agents on the initialized agents\n",
    "    # BEGIN STUDENT SOLUTION\n",
    "    #init agent\n",
    "    # get states and action sizes\n",
    "    env = gym.make('CartPole-v1')\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    agent = PolicyGradient(state_size, action_size, mode='REINFORCE', n=64)\n",
    "    graph_agents('reinfoce', agent, env, 200, 3500)\n",
    "    #agent.run(env, args.max_steps, args.num_episodes, True)\n",
    "\n",
    "    return agent\n",
    "    # END STUDENT SOLUTION\n",
    "\n",
    "\n",
    "#if '__main__' == __name__:\n",
    "agent = main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RLHW1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
